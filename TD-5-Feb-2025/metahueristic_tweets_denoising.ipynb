{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing and feature extraction libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class EnhancedDifferentialEvolutionDenoiser:\n",
    "    def __init__(self,\n",
    "                 population_size: int = 50,\n",
    "                 max_iterations: int = 100,\n",
    "                 F: float = 0.8,\n",
    "                 CR: float = 0.7,\n",
    "                 logging_level: int = logging.INFO):\n",
    "        \"\"\"\n",
    "        Initialize Enhanced Differential Evolution Denoiser for Tweet Cleaning\n",
    "\n",
    "        :param population_size: Number of solution candidates\n",
    "        :param max_iterations: Maximum iterations for optimization\n",
    "        :param F: Differential weight (mutation strength)\n",
    "        :param CR: Crossover rate\n",
    "        :param logging_level: Logging verbosity\n",
    "        \"\"\"\n",
    "        # Logging configuration\n",
    "        logging.basicConfig(level=logging_level,\n",
    "                            format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # DE algorithm parameters\n",
    "        self.population_size = population_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.F = F  # Mutation scale factor\n",
    "        self.CR = CR  # Crossover rate\n",
    "\n",
    "        # Preprocessing resources\n",
    "        self.stop_words = self._load_stop_words()\n",
    "        self.health_keywords = self._load_health_keywords()\n",
    "\n",
    "        # Language detection indicators\n",
    "        self.language_indicators = {\n",
    "            'en': ['cholera', 'outbreak', 'disease', 'water', 'hygiene'],\n",
    "            'es': ['cólera', 'brote', 'enfermedad', 'agua', 'higiene'],\n",
    "            'fr': ['choléra', 'épidémie', 'maladie', 'eau', 'hygiène']\n",
    "        }\n",
    "\n",
    "    # [Previous methods remain the same]\n",
    "    def _load_stop_words(self) -> set:\n",
    "        \"\"\"\n",
    "        Load comprehensive stop words with health domain specificity.\n",
    "\n",
    "        :return: Set of stop words\n",
    "        \"\"\"\n",
    "        basic_stop_words = {\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "            'of', 'with', 'by', 'from', 'up', 'about', 'into', 'over', 'after'\n",
    "        }\n",
    "\n",
    "        social_media_stop_words = {\n",
    "            'rt', 'via', 'http', 'https', 'www', 'retweet', 'tweet',\n",
    "            'dm', 'follow', 'unfollow'\n",
    "        }\n",
    "\n",
    "        health_stop_words = {\n",
    "            'patient', 'doctor', 'hospital', 'medical', 'health',\n",
    "            'treatment', 'medicine', 'care', 'clinic'\n",
    "        }\n",
    "\n",
    "        return basic_stop_words.union(social_media_stop_words).union(health_stop_words)\n",
    "\n",
    "    def _load_health_keywords(self, custom_path: Optional[str] = None) -> set:\n",
    "        \"\"\"\n",
    "        Load health-related keywords for cholera context.\n",
    "\n",
    "        :param custom_path: Optional path to custom keywords JSON\n",
    "        :return: Set of health-related keywords\n",
    "        \"\"\"\n",
    "        default_keywords = {\n",
    "            'cholera', 'outbreak', 'epidemic', 'water-borne', 'sanitation',\n",
    "            'hygiene', 'diarrhea', 'dehydration', 'infection', 'treatment',\n",
    "            'prevention', 'water', 'sewage', 'clean water', 'public health'\n",
    "        }\n",
    "\n",
    "        if custom_path:\n",
    "            try:\n",
    "                with open(custom_path, 'r') as f:\n",
    "                    custom_keywords = set(json.load(f))\n",
    "                default_keywords.update(custom_keywords)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not load custom keywords: {e}\")\n",
    "\n",
    "        return default_keywords\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced preprocessing for health-related social media text.\n",
    "\n",
    "        :param text: Input tweet text\n",
    "        :return: Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        # Normalize unicode characters\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remove URLs and web references\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _calculate_domain_relevance(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the domain-specific relevance score for a tweet.\n",
    "\n",
    "        :param text: Preprocessed tweet text\n",
    "        :return: Relevance score (0-1 range)\n",
    "        \"\"\"\n",
    "        # Count health-related keywords\n",
    "        keyword_matches = sum(1 for keyword in self.health_keywords if keyword in text)\n",
    "\n",
    "        # Calculate keyword density\n",
    "        words = text.split()\n",
    "        keyword_density = keyword_matches / len(words) if words else 0\n",
    "\n",
    "        return min(keyword_density * 2, 1.0)  # Cap at 1.0\n",
    "\n",
    "    def _fitness_function(self, tweets: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Fitness function to evaluate the quality of denoised tweets.\n",
    "\n",
    "        :param tweets: List of preprocessed tweets\n",
    "        :return: Fitness score (higher is better)\n",
    "        \"\"\"\n",
    "        # Calculate domain relevance for all tweets\n",
    "        relevance_scores = [self._calculate_domain_relevance(tweet) for tweet in tweets]\n",
    "\n",
    "        # Compute average relevance\n",
    "        avg_relevance = np.mean(relevance_scores)\n",
    "\n",
    "        # Penalize tweets with low domain relevance\n",
    "        penalty = len([score for score in relevance_scores if score < 0.3])\n",
    "\n",
    "        return avg_relevance - (penalty * 0.1)\n",
    "\n",
    "    def _mutate(self, population: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Mutate the population using Differential Evolution strategy.\n",
    "\n",
    "        :param population: Current population of tweet lists\n",
    "        :return: Mutated population\n",
    "        \"\"\"\n",
    "        mutated_population = []\n",
    "\n",
    "        for i in range(len(population)):\n",
    "            # Randomly select three distinct vectors\n",
    "            candidates = list(range(len(population)))\n",
    "            candidates.remove(i)\n",
    "\n",
    "            r1, r2, r3 = np.random.choice(candidates, 3, replace=False)\n",
    "\n",
    "            # Mutation strategy: DE/rand/1\n",
    "            mutant = population[r1].copy()\n",
    "\n",
    "            # Randomly choose a subset of tweets to mutate\n",
    "            num_mutate = np.random.randint(1, len(mutant))\n",
    "            mutate_indices = np.random.choice(len(mutant), num_mutate, replace=False)\n",
    "\n",
    "            for idx in mutate_indices:\n",
    "                # Apply contextual mutation based on other population members\n",
    "                candidate_tweet = population[r2][idx]\n",
    "                base_tweet = population[r3][idx]\n",
    "\n",
    "                # Perform crossover and mutation\n",
    "                if np.random.random() < self.CR:\n",
    "                    # Remove stop words\n",
    "                    candidate_words = candidate_tweet.split()\n",
    "                    base_words = base_tweet.split()\n",
    "\n",
    "                    # Combine and remove stop words\n",
    "                    mutated_words = [\n",
    "                        word for word in candidate_words + base_words\n",
    "                        if word not in self.stop_words\n",
    "                    ]\n",
    "\n",
    "                    # Reconstruct tweet\n",
    "                    mutant[idx] = ' '.join(set(mutated_words))\n",
    "\n",
    "            mutated_population.append(mutant)\n",
    "\n",
    "        return mutated_population\n",
    "\n",
    "    def _select_best_population(self,\n",
    "                                 original_population: List[List[str]],\n",
    "                                 mutated_population: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Select the best population based on fitness function.\n",
    "\n",
    "        :param original_population: Original tweet populations\n",
    "        :param mutated_population: Mutated tweet populations\n",
    "        :return: Selected population\n",
    "        \"\"\"\n",
    "        selected_population = []\n",
    "\n",
    "        for orig, mutant in zip(original_population, mutated_population):\n",
    "            orig_fitness = self._fitness_function(orig)\n",
    "            mutant_fitness = self._fitness_function(mutant)\n",
    "\n",
    "            # Select population with higher fitness\n",
    "            selected_population.append(\n",
    "                orig if orig_fitness >= mutant_fitness else mutant\n",
    "            )\n",
    "\n",
    "        return selected_population\n",
    "\n",
    "    def denoise_tweets(self, tweets: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Apply Differential Evolution for tweet denoising.\n",
    "\n",
    "        :param tweets: Input tweets\n",
    "        :return: Denoised tweets and performance metrics\n",
    "        \"\"\"\n",
    "        # Preprocess input tweets\n",
    "        valid_tweets = [tweet for tweet in tweets if tweet is not None]\n",
    "        preprocessed_tweets = [self._preprocess_text(tweet) for tweet in valid_tweets]\n",
    "\n",
    "        # Initialize population\n",
    "        population = [preprocessed_tweets.copy() for _ in range(self.population_size)]\n",
    "\n",
    "        # Differential Evolution optimization\n",
    "        best_population = population\n",
    "        best_fitness = self._fitness_function(preprocessed_tweets)\n",
    "\n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Mutation step\n",
    "            mutated_population = self._mutate(best_population)\n",
    "\n",
    "            # Selection step\n",
    "            selected_population = self._select_best_population(\n",
    "                best_population, mutated_population\n",
    "            )\n",
    "\n",
    "            # Update best population\n",
    "            current_fitness = self._fitness_function(selected_population[0])\n",
    "\n",
    "            if current_fitness > best_fitness:\n",
    "                best_population = selected_population\n",
    "                best_fitness = current_fitness\n",
    "\n",
    "                self.logger.info(f\"Iteration {iteration}: Best Fitness = {best_fitness:.4f}\")\n",
    "\n",
    "        # Calculate domain relevance scores\n",
    "        domain_relevance_scores = [\n",
    "            self._calculate_domain_relevance(tweet)\n",
    "            for tweet in best_population[0]\n",
    "        ]\n",
    "\n",
    "        # Final denoising results\n",
    "        denoising_results = {\n",
    "            'original_tweets': tweets,\n",
    "            'denoised_tweets': best_population[0],\n",
    "            'final_fitness': best_fitness,\n",
    "            'domain_relevance_scores': domain_relevance_scores\n",
    "        }\n",
    "\n",
    "        return denoising_results\n",
    "\n",
    "    def visualize_results(self, results: Dict[str, Any], output_dir: str = '.'):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualizations for denoising results.\n",
    "\n",
    "        :param results: Denoising results dictionary\n",
    "        :param output_dir: Directory to save output files\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import numpy as np\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 1. Domain Relevance Distribution (Histogram)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(results['domain_relevance_scores'], bins=10, kde=True, color='skyblue', edgecolor='black')\n",
    "        plt.title('Domain Relevance Score Distribution')\n",
    "        plt.xlabel('Domain Relevance Score')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "        # 2. Domain Relevance Box Plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=results['domain_relevance_scores'], color='lightgreen')\n",
    "        plt.title('Domain Relevance Score Box Plot')\n",
    "        plt.xlabel('Domain Relevance Score')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'domain_relevance_analysis.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Word Cloud of Denoised Tweets\n",
    "        from wordcloud import WordCloud\n",
    "\n",
    "        # Combine all denoised tweets\n",
    "        all_denoised_text = ' '.join(filter(None, results['denoised_tweets']))\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=400,\n",
    "                            background_color='white',\n",
    "                            min_font_size=10).generate(all_denoised_text)\n",
    "\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Most Frequent Words in Denoised Tweets\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.savefig(os.path.join(output_dir, 'denoised_tweets_wordcloud.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 4. Scatter plot of Original vs Denoised Tweet Length\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        orig_tweets_filtered = [tweet for tweet in results['original_tweets'] if tweet is not None]\n",
    "        orig_lengths = [len(tweet.split()) if isinstance(tweet, str) else 0 for tweet in orig_tweets_filtered]\n",
    "        denoised_lengths = [len(tweet.split()) if isinstance(tweet, str) else 0 for tweet in results['denoised_tweets']]\n",
    "\n",
    "        # Ensure both lists have the same length for the scatter plot\n",
    "        min_len = min(len(orig_lengths), len(denoised_lengths))\n",
    "        orig_lengths = orig_lengths[:min_len]\n",
    "        denoised_lengths = denoised_lengths[:min_len]\n",
    "\n",
    "        plt.scatter(orig_lengths, denoised_lengths, alpha=0.6)\n",
    "        plt.plot([min(orig_lengths), max(orig_lengths)],\n",
    "                [min(orig_lengths), max(orig_lengths)],\n",
    "                color='red', linestyle='--')\n",
    "        plt.title('Original vs Denoised Tweet Lengths')\n",
    "        plt.xlabel('Original Tweet Length (words)')\n",
    "        plt.ylabel('Denoised Tweet Length (words)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'tweet_length_comparison.png'))\n",
    "        plt.close()\n",
    "\n",
    "        orig_tweets_filtered = [tweet for tweet in results['original_tweets'] if tweet is not None] # Filter out None values\n",
    "        min_len = min(len(orig_tweets_filtered), len(results['denoised_tweets']), len(results['domain_relevance_scores'])) # Get the minimum length\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'Original Tweet': orig_tweets_filtered[:min_len], # Use filtered original tweets and slice to min_len\n",
    "            'Denoised Tweet': results['denoised_tweets'][:min_len], # Slice to min_len\n",
    "            'Domain Relevance': results['domain_relevance_scores'][:min_len]  # Slice to min_len\n",
    "        })\n",
    "        results_df.to_csv(os.path.join(output_dir, 'denoising_results.csv'), index=False)\n",
    "\n",
    "        # Print comprehensive summary\n",
    "        self._print_detailed_summary(results)\n",
    "\n",
    "    def _print_detailed_summary(self, results: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Print a comprehensive summary of the denoising results.\n",
    "\n",
    "        :param results: Denoising results dictionary\n",
    "        \"\"\"\n",
    "        # Calculate additional statistics\n",
    "        domain_relevance_scores = results['domain_relevance_scores']\n",
    "        # Filter out None and non-string values before calculating lengths\n",
    "        orig_lengths = [len(tweet.split()) for tweet in results['original_tweets'] if tweet is not None and isinstance(tweet, str)]\n",
    "        denoised_lengths = [len(tweet.split()) for tweet in results['denoised_tweets'] if tweet is not None and isinstance(tweet, str)]\n",
    "        print(\"\\n--- Comprehensive Denoising Results Summary ---\")\n",
    "        print(f\"Total Tweets Processed: {len(results['original_tweets'])}\")\n",
    "        print(f\"Final Fitness Score: {results['final_fitness']:.4f}\")\n",
    "\n",
    "        # Domain Relevance Statistics\n",
    "        print(\"\\n--- Domain Relevance Analysis ---\")\n",
    "        print(f\"Average Domain Relevance: {np.mean(domain_relevance_scores):.4f}\")\n",
    "        print(f\"Median Domain Relevance: {np.median(domain_relevance_scores):.4f}\")\n",
    "        print(f\"Highest Domain Relevance: {max(domain_relevance_scores):.4f}\")\n",
    "        print(f\"Lowest Domain Relevance: {min(domain_relevance_scores):.4f}\")\n",
    "        print(f\"Domain Relevance Standard Deviation: {np.std(domain_relevance_scores):.4f}\")\n",
    "\n",
    "        # Tweet Length Analysis\n",
    "        print(\"\\n--- Tweet Length Analysis ---\")\n",
    "        print(f\"Average Original Tweet Length: {np.mean(orig_lengths):.2f} words\")\n",
    "        print(f\"Average Denoised Tweet Length: {np.mean(denoised_lengths):.2f} words\")\n",
    "        print(f\"Median Original Tweet Length: {np.median(orig_lengths):.2f} words\")\n",
    "        print(f\"Median Denoised Tweet Length: {np.median(denoised_lengths):.2f} words\")\n",
    "\n",
    "        # Tweets with Highest and Lowest Relevance\n",
    "        print(\"\\n--- Notable Tweets ---\")\n",
    "        # Find index of highest and lowest relevance tweets\n",
    "        max_relevance_idx = domain_relevance_scores.index(max(domain_relevance_scores))\n",
    "        min_relevance_idx = domain_relevance_scores.index(min(domain_relevance_scores))\n",
    "\n",
    "        print(\"\\nHighest Relevance Tweet:\")\n",
    "        print(f\"Original: {results['original_tweets'][max_relevance_idx]}\")\n",
    "        print(f\"Denoised: {results['denoised_tweets'][max_relevance_idx]}\")\n",
    "        print(f\"Relevance Score: {domain_relevance_scores[max_relevance_idx]:.4f}\")\n",
    "\n",
    "        print(\"\\nLowest Relevance Tweet:\")\n",
    "        print(f\"Original: {results['original_tweets'][min_relevance_idx]}\")\n",
    "        print(f\"Denoised: {results['denoised_tweets'][min_relevance_idx]}\")\n",
    "        print(f\"Relevance Score: {domain_relevance_scores[min_relevance_idx]:.4f}\")\n",
    "\n",
    "    def interpret_results(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Provide a comprehensive, human-readable interpretation of the denoising results.\n",
    "\n",
    "        :param results: Denoising results dictionary\n",
    "        :return: Detailed textual interpretation of the results\n",
    "        \"\"\"\n",
    "        # Calculate key statistics\n",
    "        domain_relevance_scores = results['domain_relevance_scores']\n",
    "        # Filter out None values before calculating lengths\n",
    "        orig_lengths = [len(tweet.split()) for tweet in results['original_tweets'] if tweet is not None and isinstance(tweet, str)]\n",
    "        denoised_lengths = [len(str(tweet).split()) for tweet in results['denoised_tweets'] if tweet is not None] # Convert tweet to string if it's not\n",
    "\n",
    "        # Interpret Domain Relevance\n",
    "        avg_relevance = np.mean(domain_relevance_scores)\n",
    "        relevance_interpretation = (\n",
    "            \"Weak Relevance\" if avg_relevance < 0.3 else\n",
    "            \"Moderate Relevance\" if avg_relevance < 0.6 else\n",
    "            \"Strong Relevance\"\n",
    "        )\n",
    "\n",
    "        # Interpret Length Changes\n",
    "        avg_orig_length = np.mean(orig_lengths)\n",
    "        avg_denoised_length = np.mean(denoised_lengths)\n",
    "        length_change_pct = ((avg_denoised_length - avg_orig_length) / avg_orig_length) * 100\n",
    "\n",
    "        # Interpret Length Change\n",
    "        length_interpretation = (\n",
    "            \"Significantly Shortened\" if length_change_pct < -20 else\n",
    "            \"Moderately Shortened\" if length_change_pct < -10 else\n",
    "            \"Slightly Shortened\" if length_change_pct < 0 else\n",
    "            \"Approximately Same Length\" if abs(length_change_pct) < 10 else\n",
    "            \"Moderately Lengthened\" if length_change_pct < 20 else\n",
    "            \"Significantly Lengthened\"\n",
    "        )\n",
    "\n",
    "        # Identify Problematic Tweets\n",
    "        low_relevance_count = sum(1 for score in domain_relevance_scores if score < 0.3)\n",
    "        low_relevance_pct = (low_relevance_count / len(domain_relevance_scores)) * 100\n",
    "\n",
    "        # Construct Comprehensive Interpretation\n",
    "\n",
    "\n",
    "    def visualization_insights(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Provide insights about the visualization outputs.\n",
    "\n",
    "        :param results: Denoising results dictionary\n",
    "        :return: Textual description of visualization insights\n",
    "        \"\"\"\n",
    "        domain_relevance_scores = results['domain_relevance_scores']\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load error handling for CSV\n",
    "    def bad_line(x):\n",
    "        return None\n",
    "\n",
    "    # Load the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv('merge-csv.com__673838347b7e7.csv',\n",
    "                         encoding='unicode_escape',\n",
    "                         on_bad_lines=bad_line,\n",
    "                         engine='python',\n",
    "                         skiprows=3)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: CSV file not found. Please check the file path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract tweets\n",
    "    tweets = df['Text'].tolist()\n",
    "\n",
    "    denoiser = EnhancedDifferentialEvolutionDenoiser(\n",
    "        population_size=20,\n",
    "        max_iterations=50,\n",
    "        logging_level=logging.INFO\n",
    "    )\n",
    "\n",
    "    denoising_results = denoiser.denoise_tweets(tweets)\n",
    "\n",
    "    denoiser.visualize_results(denoising_results)\n",
    "    interpretation = denoiser.interpret_results(denoising_results)\n",
    "    print(interpretation)\n",
    "\n",
    "    viz_insights = denoiser.visualization_insights(denoising_results)\n",
    "    print(viz_insights)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVJ3rW3iIGTf",
    "outputId": "69e4ccfb-22be-4517-9d84-b1ab3360ea46",
    "ExecuteTime": {
     "end_time": "2025-03-12T09:10:21.039111Z",
     "start_time": "2025-03-12T09:06:35.243715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 11:06:41,199 - INFO: Iteration 0: Best Fitness = -1568.6477\n",
      "2025-03-12 11:06:45,165 - INFO: Iteration 1: Best Fitness = -1526.0334\n",
      "2025-03-12 11:06:48,997 - INFO: Iteration 2: Best Fitness = -1524.9318\n",
      "2025-03-12 11:06:53,050 - INFO: Iteration 3: Best Fitness = -1508.9267\n",
      "2025-03-12 11:06:56,543 - INFO: Iteration 4: Best Fitness = -1504.3249\n",
      "2025-03-12 11:07:04,117 - INFO: Iteration 6: Best Fitness = -1503.3244\n",
      "2025-03-12 11:07:07,968 - INFO: Iteration 7: Best Fitness = -1501.7242\n",
      "2025-03-12 11:07:15,528 - INFO: Iteration 9: Best Fitness = -1501.6242\n",
      "2025-03-12 11:07:18,880 - INFO: Iteration 10: Best Fitness = -1501.6240\n",
      "2025-03-12 11:07:22,442 - INFO: Iteration 11: Best Fitness = -1501.4240\n",
      "2025-03-12 11:07:30,038 - INFO: Iteration 13: Best Fitness = -1501.4240\n",
      "2025-03-12 11:10:01,248 - INFO: Iteration 46: Best Fitness = -1501.4239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comprehensive Denoising Results Summary ---\n",
      "Total Tweets Processed: 18552\n",
      "Final Fitness Score: -1501.4239\n",
      "\n",
      "--- Domain Relevance Analysis ---\n",
      "Average Domain Relevance: 0.1761\n",
      "Median Domain Relevance: 0.1333\n",
      "Highest Domain Relevance: 1.0000\n",
      "Lowest Domain Relevance: 0.0000\n",
      "Domain Relevance Standard Deviation: 0.1786\n",
      "\n",
      "--- Tweet Length Analysis ---\n",
      "Average Original Tweet Length: 25.63 words\n",
      "Average Denoised Tweet Length: 16.94 words\n",
      "Median Original Tweet Length: 23.00 words\n",
      "Median Denoised Tweet Length: 16.00 words\n",
      "\n",
      "--- Notable Tweets ---\n",
      "\n",
      "Highest Relevance Tweet:\n",
      "Original: Meanwhile cholera... https://t.co/52W58busdA\n",
      "Denoised: cholera meanwhile\n",
      "Relevance Score: 1.0000\n",
      "\n",
      "Lowest Relevance Tweet:\n",
      "Original: @K00LIP Send me some ideas!!\n",
      "Denoised: me some send ideas\n",
      "Relevance Score: 0.0000\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ]
}
